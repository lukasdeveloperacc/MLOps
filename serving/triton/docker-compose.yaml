version: '3.9'

services:
  triton-dev:
    image: nvcr.io/nvidia/tritonserver:25.06-py3
    volumes:
      - ./model_repository/hunyuan3d:/models/hunyuan3d
    ports:
      - 1234:8000
      - 1235:8001
      - 1236:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: ["tail", "-f", "/dev/null"]

  llama-vllm:
    image: nvcr.io/nvidia/tritonserver:25.06-vllm-python-py3
    environment:
      HF_TOKEN: ${HF_TOKEN}
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    working_dir: /opt/tritonserver/python/openai
    ports:
      - 9000:9000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: [ python3, openai_frontend/main.py, --model-repository, tests/vllm_models, --tokenizer, meta-llama/Meta-Llama-3.1-8B-Instruct, --tool-call-parser, llama3 ]

  # TODO
  # llama-trt:
  #   build:
  #     dockerfile: dockerfiles/LammaTRT.Dockerfile
  #   environment:
  #     HF_TOKEN: ${HF_TOKEN}
  #   volumes:
  #     - ./.cache/huggingface:/root/.cache/huggingface
  #   ports:
  #     - 9000:9000
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]
  #   command: ["tail", "-f", "/dev/null"]

  stablediffusion:
    build:
      dockerfile: dockerfiles/StableDiffusion.Dockerfile
    volumes:
      - ./model_repository/stable_diffusion:/models/stable_diffusion
    ports:
      - 1234:8000
      - 1235:8001
      - 1236:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [gpu]

  hunyuan3d:
    build:
      dockerfile: dockerfiles/Hunyuan3D.Dockerfile
    volumes:
      - ~/.cache:/root/.cache
      - ~/.cache:/root/.u2net
    ports:
      - 1237:8000
      - 1238:8001
      - 1239:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ]
              capabilities: [gpu]
